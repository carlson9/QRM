lines(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'2.5%'] + .15, type = 'l', lty = 3)
lines(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'97.5%'] + .15, type = 'l', lty = 3)
abline(h = 0, lty = 2)
cairo_pdf('~/CGPR/figures/IXIC_N225_L1_.pdf', height = 4, width = 6)
par(mfrow = c(1,1), mgp=c(1,0,0), tcl=0, mar=c(2,3,2,1))
plot(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'mean'] + .15, type = 'l',
xlab = 'Year', ylab = expression(beta), ylim = c(-.35, .35))
lines(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'2.5%'] + .15, type = 'l', lty = 3)
lines(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'97.5%'] + .15, type = 'l', lty = 3)
abline(h = 0, lty = 2)
dev.off()
cairo_pdf('~/CGPR/figures/IXIC_N225_L1_.pdf', height = 4, width = 6)
par(mfrow = c(1,1), mgp=c(1,0,0), tcl=0, mar=c(2,3,2,1))
plot(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'mean'] + .15, type = 'l',
xlab = 'Year', ylab = expression(beta), ylim = c(-.1, .35))
lines(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'2.5%'] + .15, type = 'l', lty = 3)
lines(date, summary(fit)$summary[paste0('B[3,', 1:length(date), ',3]'),'97.5%'] + .15, type = 'l', lty = 3)
abline(h = 0, lty = 2)
dev.off()
data = read.csv('~/CGPR/data/income-vs-type-of-political-regime.csv')
data$Year
max(data$Year)
?scale
rm(list=ls())
setwd('~/QRM/inclass/day10temporal/')
library(rtweets)
library(rtweet)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#rstats", n = 18000, include_rts = FALSE
)
rm(list=ls())
setwd('~/QRM/inclass/day10temporal/')
library(WDI)
WDIsearch('women labor')
WDIsearch('women')
?WDI
womenLabor = WDI(country = 'Turkey', indicator = "SL.EMP.INSV.FE.ZS")
womenLabor = WDI(country = 'TR', indicator = "SL.EMP.INSV.FE.ZS")
WDIsearch('labor force')
womenLabor = WDI(country = 'TR', indicator = "ccx_lf_pop_fem")
womenLabor = WDI(indicator = "ccx_lf_pop_fem")
womenLabor = WDI(country = 'TR', indicator = "CCX.LF.POP.FEM")
womenLabor = WDI(country = 'TR', indicator = "SL.TLF.CACT.FE.ZS")
womenLabor
womenLabor = WDI(country = 'TR', indicator = "SL.TLF.CACT.FE.ZS", start = 1990, end = 2020)
WDIsearch('gdp growth')
womenLabor = WDI(country = 'TR', indicator = c("SL.TLF.CACT.FE.ZS", "NY.GDP.MKTP.KD.ZG"), start = 1990, end = 2020)
womenLabor
womenLabor = data.frame('fLaborPart' = womenLabor$SL.TLF.CACT.FE.ZS, 'gdpg' = womenLabor$NY.GDP.MKTP.KD.ZG, 'year' = womenLabor$year)
saveRDS(womenLabor, file = 'womenLabor.rds')
#let's start with a naive test:
mod = lm(gdpg ~ fLaborPart, womenLabor)
summary(mod)
?ts
?lag
#insignificant (and negative), but we would expect LAGGED effects, right?
#many ways to do this
#let's start with base R ts() and lag()
tsWomen = ts(womenLabor, start = 1990, end = 2020, frequency = 1)
tsWomen
#now let's run a lagged effect model
modL1 = lm(gdpg ~ lag(fLaborPart), tsWomen)
summary(modL1)
#now let's run a lagged effect model
lag(tsWomen)
#now let's run a lagged effect model
lag(tsWomen$fLaborPart)
#now let's run a lagged effect model
lag(tsWomen[,'fLaborPart'])
modL1 = lm(gdpg ~ lag(fLaborPart, 1), tsWomen)
summary(modL1)
library(Hmisc)
womenLabor$fLPl1 = Lag(womenLabor$fLaborPart, -1)
womenLabor
womenLabor$fLPl1 = Lag(womenLabor$fLaborPart, +1)
womenLabor
modL1 = lm(gdpg ~ fLPl1, womenLabor)
summary(modL1)
?WDI
#gdp growth and women labor force participation
womenLabor = WDI(country = 'IN', indicator = c("SL.TLF.CACT.FE.ZS", "NY.GDP.MKTP.KD.ZG"))
womenLabor
WDIsearch('female')
WDIsearch('female')[1]
WDI(country = 'TR', indicator = 'CCX.EMPL.POP.FEM')
WDIsearch('religion')
WDIsearch('religious')
WDIsearch('religiousity')
WDIsearch('relig')
WDIsearch('healthcare')
WDIsearch('health')
WDIsearch('net refugee')
WDIsearch('refugee')
WDI(country = 'TR', indicator = 'SM.POP.REFG')
refPop = WDI(country = 'TR', indicator = 'SM.POP.REFG')
WDIsearch('population')
WDIsearch('total population')
#what is far more relevant is the proportion of the population as refugees
#we'll need total pop as well
turkeyPop = read.csv('turkeyPop.csv', header = F)
turkeyPop
turkeyPop[,2]
refPop
colnames(turkeyPop) = c('year', 'pop')
turkeyPop$pop
paste0(turkeyPop$pop)
as.numeric(turkeyPop$pop)
as.numeric(paste0(urkeyPop$pop))
as.numeric(paste0(turkeyPop$pop))
popToFix = turkeyPop$pop
popToFix
lapply(popToFix, as.character)
unlist(lapply(popToFix, as.character))
gsub(' ', '', unlist(lapply(popToFix, as.character))
)
as.numeric(gsub(' ', '', unlist(lapply(popToFix, as.character))))
turkeyPop$pop = as.numeric(gsub(' ', '', unlist(lapply(popToFix, as.character))))
head(turkeyPop)
#formatting is fixed, now to interpolate
#let's plot to get a sense
plot(turkeyPop$year, turkeyPop$pop)
?sample_values
#looks incredibly linear - no need for splines, let's just use linear interpolation
#first, let's merge the data
data = merge(refPop, turkeyPop, by = 'year')
head(data)
#looks incredibly linear - no need for splines, let's just use linear interpolation
#first, let's merge the data
data = merge(refPop, turkeyPop, by = 'year', all = T)
head(data)
library(zoo)
#this is already in the correct order, but in case it's not:
data = data[order(data$year), ]
head(data)
na.approx(data$pop)
data$pop = na.approx(data$pop)
#let's plot to get a sense again
plot(turkeyPop$year, turkeyPop$pop)
#let's plot to get a sense again
plot(data$year, data$pop)
#now let's get proportions
data$propRef = data$SM.POP.REFG/data$pop
head(data)
#let's get a sense of the dist
plot(density(data$propRef))
data$propRef
turkeyPop
refPop
#2021 not in refugee data, so drop
data = data[data$year != 2021, ]
#let's get a sense of the dist
plot(density(data$propRef))
plot(data$year, data$propRef)
#obviously, after 2011 huge spike; we don't need a model to tell us this (or why)
#let's drop after 2011 and think about other patterns
data = data[data$year <= 2011, ] #again, many ways
#plot again
plot(data$year, data$propRef)
#now we see more interesting variation
#let's look at another density plot
plot(density(data$propRef))
#now, transform to be continuous
#first, log
data$propRefLog = log(data$propRef)
plot(density(data$propRefLog))
#now, scale
data$propRefLogSc = scale(data$propRefLog)
plot(density(data$propRefLogSc))
#over time
plot(data$year, data$propRefLogSc)
WDIsearch('conser')
WDIsearch('ideo')
WDIsearch('open')
WDIsearch('refugee')
#what could explain this, other than exogenous shocks?
#[1,] "DT.ODA.DACD.RFGE.CD" "Gross ODA aid disbursement for refugees in donor countries,  DAC donors total (current US$)"
monetaryData = WDI(country = 'TR', indicator = 'DT.ODA.DACD.RFGE.CD')
WDIsearch('aid')
WDIsearch('aid')[89]
WDIsearch('aid')[89,]
#what could explain this, other than exogenous shocks?
#"DT.ODA.ALLD.CD" "Net official development assistance and official aid received (current US$)"
monetaryData = WDI(country = 'TR', indicator = 'DT.ODA.ALLD.CD')
monetaryData
#what could explain this, other than exogenous shocks?
#"DT.ODA.ALLD.CD" "Net official development assistance and official aid received (current US$)"
monetaryData = WDI(country = 'TR', end = 2011, indicator = 'DT.ODA.ALLD.CD')
monetaryData
#go ahead and scale
monetaryData$aidSc = scale(monetaryData$DT.ODA.ALLD.CD)
data = merge(data, monetaryData, by = 'year')
data
#overlay a plot of aid
points(data$year, data$aidSc)
#over time
plot(data$year, data$propRefLogSc)
#overlay a plot of aid
points(data$year, data$aidSc, pch = '+')
#definitely seems like a correlation
#let's naively test
mod = lm(propRefLogSc ~ aidSc, data)
summary(mod)
#we could actually drop the intercept (notice it's close to zero) because both are scaled
#we will leave it in though, because we ultimately care about temporality
#we could frame two hypotheses:
#refugees increase with aid
#aid increases refugee admittance
#pick the hypothesis, and lag the explanatory variable
#I imagine (and looking at the plot think) that aid is a function of refugee admittance, not the other way
library(Hmisc)
data$propRefLogScL1 = Lag(data$propRefLogSc, +1)
head(data)
#now model
modL1 = lm(aidSc ~ propRefLogScL1, data)
summary(modL1)
#but what if we hypothesized the other direction?
data$aidScL1 = Lag(data$aidSc, +1)
#now model
mod2L1 = lm(propRefLogSc ~ aidScL1, data)
summary(mod2L1)
#I'm saving the data in case the indicators change (good practice)
saveRDS(data, 'refAid.rds')
#now let's check assumptions of first model
acf(modL1)
#now let's check assumptions of first model
acf(mod)
#now let's check assumptions of first model
acf(data$aidSc)
pacf(data$aidSc)
kpss.test(data$aidSc,
null = 'Trend')
library(tseries)
kpss.test(data$aidSc,
null = 'Trend')
kpss.test(data$aidSc)
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc)
?adf.test
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, k=1)
adf.test(data$aidSc, alternative = 'stationary')
adf.test(data$aidSc, alternative = 'stationary', k=1)
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc) #no problems
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, k=0) #no problems
adf.test(data$aidSc, alternative = 'stationary', k=0)
adf.test(data$aidSc, alternative = 'explosive', k=0)
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc) #hmm...
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc, k = 0) #hmm...
?kpss.test
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc) #hmm...
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, k=0) #no problems
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, alternative = 'explosive', k=0) #no problems
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc, null = 'trend') #hmm...
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc, null = 'Trend') #hmm...
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc, null = 'Trend', k = 0) #hmm...
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test
kpss.test(data$aidSc, null = 'Level') #hmm...
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, alternative = 'explosive', k=0) #no problems
?adf.test
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, alternative = 'stationary', k=0) #no problems
adf.test(data$aidSc, alternative = 'explosive', k=0) #no problems
x <- rnorm(1000)  # no unit-root
adf.test(x)
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, k=0)
#(Augmented) Dickey Fuller test (more common, but simply worse)
adf.test(data$aidSc, alternative = 'stationary', k=0) #
adf.test(data$aidSc, alternative = 'explosive', k=0) #no problems
#instead of exploring lags, let's use auto.arima()
#Returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided.
library(forecast)
auto.arima(data$aidSc)
?arima
#ok, we should include a lagged DV as well
modARIMA = lm(aidSc ~ propRefLogScL1 + aidScL1, data)
summary(modARIMA)
#what about instantaneous and lagged IV?
modWTF = lm(aidSc ~ propRefLogSc + propRefLogScL1, data)
summary(modWTF)
#even more 'robust' (but useless for inference, almost)
modWTF2 = lm(aidSc ~ propRefLogSc + propRefLogScL1 + aidScL1, data)
summary(modWTF2)
#let's omit the unlagged IV
modWTF3 = lm(aidSc ~ propRefLogScL1 + aidScL1, data)
summary(modWTF3)
#return to the original model
summary(mod)
#return to the original L1 model
summary(modL1)
#let's adjust the standard errors based on temporality (Newey West - I have written about SE adjustments; they are common, but not terribly good)
library(sandwich)
library(lmtest)
coeftest(modL1,
vcov. = NeweyWest(modL1,
order.by = ~data$year))
data$year
colnames(data)
#we need to drop NAs manually
modL1data = na.omit(data.frame('aidSc' = data$aidSc, 'propRefLogScL1' = data$propRefLogScL1, 'year' = data$year))
modL1x = lm(aidSc ~ propRefLogScL1, modL1data)
coeftest(modL1x,
vcov. = NeweyWest(modL1x,
order.by = ~modL1data$year))
#return to the original L1 model
summary(modL1)
#now let's analyze journalist imprisonments in Turkey before and after the recent coup attempt
#this is much more likely the type of hypotheses you'll be testing in TS
#now bounded and discrete data in the outcome
# Scraping
library("rvest")
# Melting
library("data.table")
# Piping
library("magrittr")
# Manipulating
library("dplyr")
# Plotting
library("ggplot2")
# Styling
library("ggthemes")
# Styling
library("ggthemes")
url <- "https://en.wikipedia.org/wiki/List_of_arrested_journalists_in_Turkey"
kept_columns <- c("Date arrested")
webpage <- read_html(url)
tbls <- html_nodes(webpage, "table")
# Shows all tables
tbls
# If more matched, find search term
tbls[grep("Arrested",tbls,ignore.case = T)]
df <- html_table(tbls[grep("Arrested",tbls,ignore.case = T)],fill = T)[[1]]
df
arrestDate = df[, kept_columns]
arrestDate
as.POSIXct(arrestDate,
format = '%Y:%M:%D')
as.POSIXct(as.character(arrestDate),
format = '%Y:%M:%D')
arrestDate
arrestDate[1:144]
arrestDate[1:144,]
as.POSIXct(as.character(arrestDate),
format = '%Y-%M-%D')
?as.POSIXct.IDate
as.POSIXct(as.character(arrestDate),
format = '%Y-%M-%D', ms = 'truncate')
gsub('[+]', '', arrestDate)
gsub('/[+/]', '', arrestDate)
gsub("\\s*\\([^\\)]+\\)","",as.character(arrestDate))
gsub("\\s*\\[[^\\]+\\)","",as.character(arrestDate))
gsub("\\s*\\[[^\\]+\\)","",arrestDate)
lapply(arrestDate, gsub, "\\s*\\[[^\\]+\\)","")
lapply(arrestDate, function(x) gsub("\\s*\\[[^\\]+\\)","",x))
lapply(arrestDate, function(x) gsub("\\[[^\\])","",x))
lapply(arrestDate, function(x) gsub("\\[^\\])","",x))
sub(" \\[.*", "", arrestDate$`Date arrested`)
sub(" [.*", "", arrestDate$`Date arrested`)
sub("\[.*", "", arrestDate$`Date arrested`)
sub("\\[.*", "", arrestDate$`Date arrested`)
arrestDate = sub("\\[.*", "", arrestDate$`Date arrested`)
arrestDate
as.POSIXct(as.character(arrestDate),
format = '%Y-%M-%D', ms = 'truncate')
as.POSIXct(as.character(arrestDate),
format = '%Y-%M-%D')
as.POSIXct(arrestDate,
format = '%Y-%M-%D')
arrestDate
as.Date(arrestDate,
format = '%Y-%M-%D')
as.Date(arrestDate,
format = 'YYYY-MM-DD')
as.Date(arrestDate[1],
format = '%Y-%M-%D')
arrestDate[1]
as.Date(arrestDate[1])
as.Date(arrestDate)
arrestDataHold = as.Date(arrestDate)
arrestDateHold = as.Date(arrestDate)
arrestDate[is.na(arrestDataHold)]
arrestDateHold[is.na(arrestDataHold)] = c('2009-03-24', '', '', '', '', '', '2017-06-14', '2016-10-01', '', '2017-04-01', '2017-03-01', '')
arrestDateHold
class(arrestDate)
class(as.Date(arrestDate))
arrestDate = as.Date(arrestDateHold)
rrestDate
arrestDate
month(arrestDate)
quarter(arrestDate)
year(arrestDate)
arrestYear = year(arrestDate)
frequency(arrestYear)
table(arrestYear)
arrestYear = arrestYear[arrestYear >= 1994]
plot(names(table(arrestYear)), table(arrestYear))
years = names(table(arrestYear))
freqY = table(arrestYear)
plot(years, freqY)
abline(v = 2016, lty = 2)
plot(years, freqY)
abline(v = 2015.5, lty = 2)
#was 2016 a statistical anamoly?
#segmented
library(segmented)
fit_segmented = segmented(fit_lm, seg.Z = ~years, npsi = 1:5)  # 1:5 change points along x
#was 2016 a statistical anamoly?
#let's start with agnostically looking for changepoints
#segmented
library(segmented)
fit_lm = lm(freqY ~ 1 + years)  # intercept-only model
fit_segmented = segmented(fit_lm, seg.Z = ~years, npsi = 1:5)  # 1:5 change points along x
fit_segmented_list = lapply(1:5, function(x) segmented(fit_lm, seg.Z = ~years, npsi = x))  # 1:5 change points along x
fit_segmented_list
x
segmented(fit_lm, seg.Z = ~years, npsi = x)
segmented(fit_lm, seg.Z = ~years, npsi = 1)
years
years = as.numeric(names(table(arrestYear)))
table(arrestYear)
freqY = table(arrestYear)
plot(years, freqY)
abline(v = 2015.5, lty = 2)
#was 2016 a statistical anamoly?
#let's start with agnostically looking for changepoints
#segmented
library(segmented)
fit_lm = lm(freqY ~ 1 + years)  # intercept-only model
fit_segmented_list = lapply(1:5, function(x) segmented(fit_lm, seg.Z = ~years, npsi = x))  # 1:5 change points along x
summary(fit_segmented_list)
segmented(fit_lm, seg.Z = ~years, npsi = 1)
summary(segmented(fit_lm, seg.Z = ~years, npsi = 1))
fit_segmented_list = lapply(1:5, function(bp) segmented(fit_lm, seg.Z = ~years, npsi = bp))  # 1:5 change points along x
summary(fit_segmented_list[[1]])
lapply(fit_segmented_list, summary)
#2 cut-offs seems appropriate
plot(fit_segmented_list[[2]])
points(years, freqY)
lines.segmented(fit_segmented_list[[2]])
points.segmented(fit_segmented_list[[2]])
lapply(fit_segmented_list, BIC)
#3 cut-offs seems appropriate
plot(fit_segmented_list[[3]])
#3 cut-offs seems appropriate
plot(fit_segmented_list[[3]])
fit_segmented_list[[3]]
fit_segmented_list = lapply(1:5, function(bp) segmented(fit_lm, seg.Z = ~years, npsi = bp))  # 1:5 change points along x
lapply(fit_segmented_list, summary)
lapply(fit_segmented_list, BIC)
#3 cut-offs seems appropriate
plot(fit_segmented_list[[3]])
#3 cut-offs seems appropriate
plot(fit_segmented_list[[1]])
points(years, freqY)
#3 cut-offs seems appropriate
plot(fit_segmented_list[[2]])
#3 cut-offs seems appropriate
plot(fit_segmented_list[[3]])
lapply(fit_segmented_list, summary)
lapply(fit_segmented_list, BIC)
#1 cut-off seems appropriate (because the third could not estimate 3 cut-points)
plot(fit_segmented_list[[1]])
points(years, freqY)
lines.segmented(fit_segmented_list[[1]])
points.segmented(fit_segmented_list[[1]])
#same, but a slightly different library that optimizes fit stats for us
library(strucchange)
#same, but a slightly different library that optimizes fit stats for us
library(strucchange)
fit_bp = breakpoints(freqY ~ 1, breaks = 5)
summary(fit_bp)
fit_bp = breakpoints(freqY ~ 1 + years, breaks = 5)
fit_bp = breakpoints(freqY ~ 1, breaks = 5)
summary(fit_bp)
#bcp: Bayesian; automatically detects change points and segment types, though you can use the parameter d to increase the prior probability of intercept-only models
# It provides estimates of means and probability of change point at each x-coordinate.
library(bcp)
#bcp: Bayesian; automatically detects change points and segment types, though you can use the parameter d to increase the prior probability of intercept-only models
# It provides estimates of means and probability of change point at each x-coordinate.
library(bcp)
fit_bcp = bcp(freqY, d = 1000)
fit_bcp = bcp(freqYdata$freqY, d = 1000)
freqYdata = data.frame('freqY' = freqY, 'years' = years)
fit_bcp = bcp(freqYdata$freqY, d = 1000)
freqYdata$freqY
freqYdata
fit_bcp = bcp(freqYdata$freqY.Freq, d = 1000)
plot(fit_bcp)
fit_lm = glm(freqY ~ 1 + years, link = 'poisson')  # intercept-only model
fit_p = glm(freqY ~ 1 + years, family = 'poisson')  # intercept-only model
#what about a link function? we'll use Poisson
fit_p = glm(freqY ~ 1 + years, family = 'poisson')  # intercept-only model
fit_segmented_list = lapply(1:5, function(bp) segmented(fit_p, seg.Z = ~years, npsi = bp))  # 1:5 change points along x
lapply(fit_segmented_list, summary)
lapply(fit_segmented_list, BIC)
#now 4 cut-offs seems appropriate (because the third could not estimate 3 cut-points)
plot(fit_segmented_list[[4]])
points(years, freqY)
lines.segmented(fit_segmented_list[[4]])
points.segmented(fit_segmented_list[[4]])
