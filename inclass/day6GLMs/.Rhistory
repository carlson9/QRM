observeEvent(input$ok, {
removeModal()
})
# for reading the file
output$input_file = renderTable({
file_to_read=input$file
if(is.null(file_to_read)){
return()
}
read.table(file_to_read$datapath, sep=input$sep, header=input$header)
})
selectText = function(row){
showModal(modalDialog(
radioButtons('choice', 'Select one document',
choiceNames = c(txt1[row],
txt2[row]),
choiceValues = c(1,0),
selected = character(0)),
footer = tagList(
actionButton("ok", "OK")
)
))
}
observeEvent(input$submit, {
tt <<- read.table(input$file$datapath, sep=input$sep, header=input$header)
if(!is.null(input$rID)){
tt <<- cbind(input$rID, tt)
colnames(tt) <<- c('ResearcherID', 'text1', 'text2')
}else{
colnames(tt) <<- c('text1', 'text2')
}
txt1 <<- as.character(tt[,1])
txt2 <<- as.character(tt[,2])
hold = selectText(1)
print(hold)
})
}
# Create Shiny object
shinyApp(ui = ui, server = server)
knitr::opts_chunk$set(echo = TRUE)
setwd('~/QPMRFall2021/inclass/day2Assump/')
#we'll use tidyverse this time
library(tidyverse)
library(caret)
theme_set(theme_classic())
# Load the data
data("Boston", package = "MASS")
head(Boston)
Boston$medv
training.samples <- Boston$medv %>%
createDataPartition(p = 0.8, list = FALSE)
training.samples
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
head(train.data)
head(test.data)
?createDataPartition
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth()
# Build the model
model <- lm(medv ~ lstat, data = train.data)
model
# Make predictions
predictions <- model %>% predict(test.data)
predictions
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth(method = lm, formula = y ~ x)
# Squaring
model2 = lm(medv ~ poly(lstat, 2, raw = TRUE), data = train.data)
# 6 degree polynomial
lm(medv ~ poly(lstat, 6, raw = TRUE), data = train.data) %>%
summary()
# Drop the sixth
# Build the model
model3 <- lm(medv ~ poly(lstat, 5, raw = TRUE), data = train.data)
# Make predictions
predictions <- model3 %>% predict(test.data)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE))
# Log transformation
# Build the model
model4 <- lm(medv ~ log(lstat), data = train.data)
# Make predictions
predictions <- model4 %>% predict(test.data)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth(method = lm, formula = y ~ log(x))
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
library(splines)
# Build the model
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
model
summary(model)
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 3))
library(mgcv)
# Build the model
model <- gam(medv ~ s(lstat), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth(method = gam, formula = y ~ s(x))
library(dynlm)
library(AER)
data("USMacroG")
head(USMacroG)
?USMacroG
?L
?L()
mod.dyn = dynlm(consumption ~ dpi + L(dpi), data = USMacroG)
summary(mod.dyn)
durbinWatsonTest(mod.dyn)
durbinWatsonTest(mod.dyn, max.lag = 4)
setwd('~/QPMRFall2021/inclass/day2Assump/')
load('merged.Rdata') #load the data
mergedY = merged[!is.na(merged$avgAA),]
library(itsadug)
mergedY
mergedY = start_event(mergedY, column="year", event='pais', label.event="Event")
mergedY
m1 <- bam(avgAA ~ te(year)+s(inflation), data = mergedY)
summary(m1)
acf(resid(m1))
acf(resid_gam(m1))
acf_resid(m1)
r1 <- start_value_rho(m1, plot=TRUE)
r1
m1AR1 <- bam(avgAA ~ te(year)+s(inflation), data=mergedY, rho=r1, AR.start=mergedY$start.event)
summary(m1AR1)
acf_resid(m1)
acf_resid(m1AR1)
#model sentiment towards US as a function of inflation, with theoretical controls
mod = lm(avgAA ~ inflation + exports + imports + aid + propEmig, data = mergedY)
#model sentiment towards US as a function of inflation, with theoretical controls
mod = lm(avgAA ~ inflation + exports + imports + aid + propEmig, data = mergedY)
summary(mod)
vif(mod)
#exports and imports are very high
mod.vif = lm(avgAA ~ inflation + exports + aid + propEmig,
data = mergedY)
summary(mod.vif)
vif(mod.vif)
#Bonferroni outlier test
outlierTest(mod)
mod.out = lm(avgAA ~ inflation + aid + propEmig + exports + imports, mergedY[-81,])
summary(mod.out)
#always set a seed when simulating data
set.seed(1454)
#draw from a standard normal
rnorm(1)
#draw 10 from a standard normal
rnorm(10)
#plot a density
plot(density(rnorm(10)))
plot(density(rnorm(10000)))
#change the mean and standard deviation
plot(density(rnorm(10000, mean = 5, sd = 3)))
#normal cumulative density function
pnorm(-Inf)
pnorm(Inf)
pnorm(0)
plot(pnorm(seq(-3, 3, by = .01)))
plot(pnorm(seq(-3, 3, by = .01)), type = 'l')
plot(seq(-3, 3, by = .01), pnorm(seq(-3, 3, by = .01)), type = 'l')
#normal probability density function
dnorm(-Inf)
dnorm(Inf)
dnorm(0)
plot(dnorm(seq(-3, 3, by = .01)))
plot(seq(-3, 3, by = .01), dnorm(seq(-3, 3, by = .01)), type = 'l')
plot(seq(-3, 3, by = .01), pnorm(seq(-3, 3, by = .01)), type = 'l')
#normal inverse CDF
qnorm(.5)
qnorm(.025)
qnorm(1 - .025)
qnorm(.025, lower = TRUE)
qnorm(.025, lower = FALSE)
plot(seq(0, 1, by = .01), qnorm(seq(0, 1, by = .01)), type = 'l')
#gamma pdf
rgamma(1, shape = 1)
plot(density(rgamma(10000, shape = 1)))
plot(density(rgamma(10000, shape = 10)))
plot(seq(0, 10, by = .01), dgamma(seq(0, 10, by = .01), shape = 1), type = 'l')
#gamma CDF
plot(seq(0, 10, by = .01), pgamma(seq(0, 10, by = .01), shape = 1), type = 'l')
#binomial
rbinom(1, 1, .5)
#binomial
rbinom(1, 1, .5)
#binomial
rbinom(1, 1, .5)
#binomial
rbinom(1, 1, .5)
rbinom(1, 10, .5)
rbinom(1, 10, .5)
rbinom(1, 10, .5)
rbinom(n = 100, size = 10, prob = .5)
#sampling
sample(c(0,1), 1, prob = c(.8, .2))
#sampling
sample(c(0,1), 1, prob = c(.8, .2))
#sampling
sample(c(0,1), 1, prob = c(.8, .2))
#sampling
sample(c(0,1), 1, prob = c(.8, .2))
sample(c(0,1), 100, prob = c(.8, .2), replace = TRUE)
#uniform
runif(100)
#Weibull
rweibull(1, shape = 1, scale = 1)
rweibull(1, shape = 10, scale = 10)
#sample from uniform to generate any distribution using the inverse CDF
u = runif(10000)
u
samps = qweibull(u, 10, 10)
plot(density(samps))
plot(density(rweibull(10000, 10, 10)))
plot(density(qnorm(u)))
toeplitz((10:1)/10)
#generate random variance-covariance matrices
rWishart(1, 10, toeplitz((10:1)/10))
#generate random variance-covariance matrices
rWishart(1, 10, toeplitz((10:1)/10))
sig = rWishart(1, 10, toeplitz((10:1)/10))
sig
#mutlivariate normal
library(mgcv)
samps = rmvn(1000, mu = rep(0, 10), V = sig[,,1])
samps
rmvn(1, mu = rep(0, 10), V = sig[,,1])
sig[,,1]
dim(samps)
cov(samps)
sig
#generate auto-regressive variables
rho = .8
n = 5
sig = diag(1, n)
for(i in 1:n){
for(k in 1:n){
sig[i, k] = sig[k, i] = rho^(i-k)
}
}
sig
sig
samps = rmvn(1000, mu = rep(0, n), V = sig)
cov(samps)
samps
cov(samps)
#convert to a correlation matrix
library(MBESS)
cov2cor(cov(samps))
cov(samps)
rWishart(1, 10, toeplitz((10:1)/10))
#generate more explicit correlations
#if x1 and x2 are correlated rho, x1 ~ rho*x2 + sqrt(1-rho^2)*F,
#   where F is the distribution
rho = .6
x1 = rnorm(1000)
x1
x2 = rho * x1 + sqrt(1 - rho^2)*rnorm(1000)
cor(x1, x2)
#autocorrelation
err = rnorm(1)
n = 20
for(i in 2:n) err[i] = rho * err[i - 1] + sqrt(1 - rho^2)*rnorm(1)
plot(1:20, err, type = 'l')
mean(err)
sd(err)
err.scaled = scale(err)
plot(1:20, err.scaled, type = 'l')
mean(err.scaled)
sd(err.scaled)
#remember the gamma
plot(seq(0, 10, by = .01), dgamma(seq(0, 10, by = .01), shape = 10), type = 'l')
?lappy
?lapply
samps = lapply(1:1000, function(x) rgamma(1000, shape = 10))
class(samps)
samps
means = unlist(lapply(samps, mean))
means
plot(density(means))
samps.bar = mean(unlist(samps))
sig = var(unlist(samps))
means.normalized = (means - samps.bar)/(sqrt(sig / 1000))
plot(density(means.normalized))
#what about binomial?
plot(density(rbinom(1000, 1, .8)))
samps = lapply(1:1000, function(x) rbinom(1000, size = 1, prob = .8))
class(samps)
means = unlist(lapply(samps, mean))
plot(density(means))
samps.bar = mean(unlist(samps))
sig = var(unlist(samps))
means.normalized = (means - samps.bar)/(sqrt(sig / 1000))
plot(density(means.normalized))
#are they from a standard normal?
t.test(means.normalized, rnorm(1000))
shapiro.test(means.normalized)
shapiro.test(rnorm(1000))
#non-parametric
ks.test(means.normalized, rnorm(1000))
?ks.test
rgamma(10)
rgamma(10, 10)
rnorm(9)
rbinom(5)
rbinom(5, 5)
rbinom(5, 5, .9)
setwd('~/QPMRFall2021/inclass/day5GLM/')
getwd()
setwd('~/QPMRFall2021/inclass/day5MLE/')
library(readstata13)
data = read.dta13('repdata.dta')
head(data)
#logit (most common)
mod1 = glm(war ~ Oil + empgdpenl + emplpopl + empolity2l,
data = data,
family = binomial(link = 'logit'))
summary(mod1)
#probit
mod2 = glm(war ~ Oil + empgdpenl + emplpopl + empolity2l,
data = data,
family = binomial(link = 'probit'))
summary(mod2) #notice the different scale of the estimates
#c-log-log (very uncommon)
mod3 = glm(war ~ Oil + empgdpenl + emplpopl + empolity2l,
data = data,
family = binomial(link = 'cloglog'))
summary(mod3)
#Poisson
mod4 = glm(wars ~ empolity2l + empgdpenl + emplpopl + ethfrac + relfrac,
data = data,
family = poisson(link = 'log'))
summary(mod4)
#negative binomial (for overdispersed)
library(MASS)
mod5 = glm.nb(wars ~ empolity2l + empgdpenl + emplpopl + ethfrac + relfrac,
data = data)
summary(mod5) #notice the warnings
warnings()
mod5 = glm.nb(wars ~ empolity2l + empgdpenl + emplpopl + ethfrac + relfrac,
data = data,
control = glm.control(maxit = 20, epsilon = 1e-8))
summary(mod5)
#logistic multinomial (using a neural net)
library(nnet)
mod6 = multinom(region ~ ethfrac + gdptype,
data = data)
summary(mod6)
#logistic multinomial (MLE)
library(mlogit)
?mlogit #this library is a huge pain, so I suggest fitting with nnet
data("Fishing", package = "mlogit")
Fish <- mlogit.data(Fishing, varying = c(2:9), shape = "wide", choice = "mode")
summary(mlogit(mode ~ price + catch, data = Fish))
#ordered logistic regression
#just for illustration, well repeat the number of wars, but this is inappropriate because it is theoretically unbounded
library(MASS)
mod7 = polr(as.factor(wars) ~ empolity2l + empgdpenl + emplpopl + ethfrac + relfrac,
data = data)
summary(mod7)
#quasibinomial regression for proportions
#predict ethnic frac as a function of polity
mod8 = glm(ethfrac ~ empolity2l,
data = data,
family = quasibinomial)
summary(mod8)
rm(list=ls())
if(dirname(rstudioapi::getActiveDocumentContext()$path) != '') setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
final_df = read.csv('data.csv')
head(final_df)
#running a simple regression on intercommunal violence (not violence with the state):
pop_logit <- glm(intercon ~ country_pop + aggdifxx + gdppc + polity2,
data=final_df,
family = binomial)
summary(pop_logit)
#lets get a tex table
library(stargazer)
stargazer(pop_logit)
#we might prefer Word or HTML
library(jtools)
export_summs(pop_logit, to.file = 'docx')
#exponentiated coefficients
export_summs(pop_logit, exp = T)
#plot summarization
plot_summs(pop_logit)
#exponentiate
plot_summs(pop_logit, exp = T)
#let's get predicted probabilities with SEs
probs = predict(pop_logit, type = 'response', se.fit = T)
#add uncertainty
lowers = probs$fit - 1.96*probs$se.fit
uppers = probs$fit + 1.96*probs$se.fit
newData = na.omit(final_df[, c('intercon', 'country_pop', 'aggdifxx', 'gdppc', 'polity2')])
plot(probs$fit ~ newData$country_pop, pch = 18)
segments(x0 = newData$country_pop, x1 = newData$country_pop,
y0 = lowers, y1 = uppers)
#very ugly, and not that meaningful
#instead, take the mean of other variables, and allow country_pop to vary
dataToPlot = data.frame('country_pop' = seq(min(newData$country_pop), max(newData$country_pop), length.out = 1000),
'aggdifxx' = mean(newData$aggdifxx),
'gdppc' = mean(newData$gdppc),
'polity2' = mean(newData$polity2))
probs = predict(pop_logit, newdata = dataToPlot, type = 'response', se.fit = T)
lowers = probs$fit - 1.96*probs$se.fit
uppers = probs$fit + 1.96*probs$se.fit
plot(probs$fit ~ dataToPlot$country_pop, type = 'n')
points(dataToPlot$country_pop, lowers, lty = 2, type = 'l')
points(dataToPlot$country_pop, uppers, lty = 2, type = 'l')
polygon(c(rev(dataToPlot$country_pop), dataToPlot$country_pop), c(rev(uppers), lowers), col = 'grey80', border = NA)
points(probs$fit ~ dataToPlot$country_pop, type = 'l')
#add a rug if you want
rug(newData$country_pop)
#in papers, you generally want an intuitive numerical explanation
#not that important for logits, cause of exp, but for GLMs in general
#we take the mean prediction, and subtract the sd
diff(predict(pop_logit, newdata = data.frame(
'country_pop' = c(mean(newData$country_pop), mean(newData$country_pop) - sd(newData$country_pop)),
'aggdifxx' = mean(newData$aggdifxx),
'gdppc' = mean(newData$gdppc),
'polity2' = mean(newData$polity2)
), type = 'response'))
#now matched cases
library(MatchIt)
match.out = matchit(I(country_pop > mean(country_pop)) ~
aggdifxx + gdppc + polity2,
data = newData,
method = 'nearest', distance = 'mahalanobis',
replace = T)
matches = as.numeric(match.out$match.matrix)
matches2 = as.numeric(row.names(match.out$match.matrix))
match.out
match.out$match.matrix
matches = as.numeric(match.out$match.matrix)
matches2 = as.numeric(row.names(match.out$match.matrix))
mean(final_df[matches2, 'intercon'] - final_df[matches, 'intercon'])
final_df[matches2[1], 'intercon'] - final_df[matches[1], 'intercon'] #way more meaningful when it's not binary
#now we compare to the null model to check for explanatory power
pop_logitNull <- glm(intercon ~ aggdifxx + gdppc + polity2,
data=final_df,
family = binomial)
#analysis of variance
#we use a chi-squared test here
anova(pop_logit, pop_logitNull, test = 'Chisq')
#two models in one call
stargazer(pop_logit, pop_logitNull)
con_logit <- glm(intercon ~ groupcon + aggdifxx + gdppc + polity2,
data=final_df,
family = binomial)
summary(con_logit)
anova(pop_logit, con_logit)
#now running the ordered logit regression on rebellion score
library(MASS)
pop_olr <- polr(as.factor(rebellion) ~ country_pop + aggdifxx + gdppc + polity2,
data=final_df)
summary(pop_olr)
con_olr <- polr(as.factor(rebellion) ~ groupcon + aggdifxx + gdppc + polity2,
data=final_df)
summary(con_olr)
anova(pop_olr, con_olr)
pop_olrNull <- polr(as.factor(rebellion) ~ aggdifxx + gdppc + polity2,
data=final_df)
anova(pop_olr, pop_olrNull)
#plot the results of a polr
newData = na.omit(final_df[, c('rebellion', 'country_pop', 'aggdifxx', 'gdppc', 'polity2')])
probs = as.data.frame(predict(pop_olr, newData, se.fit = TRUE, type = 'probs'))
plot(0:8, probs[1,], type = 'b', pch = 18)
for(i in 2:nrow(newData)) points(0:8, probs[i,], type = 'b', pch = 18)
